{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00000-0cbdb4a7-8fc8-4afb-91f0-94aea85cb8c9",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "53138ffb",
    "execution_start": 1640269196879,
    "execution_millis": 4954,
    "deepnote_cell_type": "code"
   },
   "source": "# Useful starting lines\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n#The usual collection of indispensables \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nfrom tqdm import tqdm\nimport scipy.fftpack\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error, explained_variance_score, max_error\nfrom sklearn.model_selection import cross_val_score, train_test_split",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00001-53cb0e9c-7002-4b3e-8b7a-c2413f60a476",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ed72612c",
    "execution_start": 1640269201840,
    "execution_millis": 479,
    "deepnote_cell_type": "code"
   },
   "source": "from helpers import sample_data, load_data\n\n# load data.\ndata_oligo_1 = np.delete(load_data(\"data-oligo/011021_SFL_SYN211_Oligo_1uM_Rawdata_270spectralcolumns.csv\"), 0, 1).T\ndata_oligo_1 = data_oligo_1[data_oligo_1[:, 0] != -999, :]\ndata_oligo_2 = np.delete(load_data(\"data-oligo/051021_SFL_SYN211_Oligo_5uM_rawdata_270spectralcolumns.csv\"), 0, 1).T\ndata_oligo_2 = data_oligo_2[data_oligo_2[:, 0] != -999, :]\ndata_oligo = np.append(data_oligo_1, data_oligo_2, axis = 0)\ny_oligo = np.expand_dims(np.zeros(len(data_oligo)), axis=1)\n\ndata_PFF1 = np.delete(load_data(\"data-pff/191121_G80_AInII_SYn211_AsynPFF_5microM_rawdata_290spectracolumns.csv\"), 0, 1).T\ndata_PFF1 = data_PFF1[data_PFF1[:, 0] != -999, :]\ndata_PFF2 = np.delete(load_data(\"data-pff/220421_G80_AInII_SYn211_AsynPFF_20microM_880_spectralcolumns.csv\"), 0, 1).T\ndata_PFF2 = data_PFF2[data_PFF2[:, 0] != -999, :]\ndata_PFF =  np.append(data_PFF1, data_PFF2, axis = 0)\ny_PFF = np.expand_dims(np.ones(len(data_PFF)), axis=1)\n\nprint(data_oligo.shape, data_PFF.shape)",
   "outputs": [
    {
     "name": "stdout",
     "text": "(540, 133) (1170, 133)\n",
     "output_type": "stream",
     "data": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00002-dc017ff8-e611-4d08-8aeb-4d07557319f6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2b58a87a",
    "execution_start": 1640269202326,
    "execution_millis": 17,
    "deepnote_cell_type": "code"
   },
   "source": "#Build X and y by concatenating the different dataset (after sampling the same number of datapoints for each dataset)\nnp.random.seed(2)\nX = np.concatenate((data_oligo[np.random.randint(data_oligo.shape[0], size=540),:], data_PFF[np.random.randint(data_PFF.shape[0], size=540),:]), axis = 0)\ny = np.concatenate((y_oligo[np.random.randint(y_oligo.shape[0], size=540),:], y_PFF[np.random.randint(y_PFF.shape[0], size=540),:]), axis = 0)\n\n# Train and test\nx_train, _, y_train, _ = train_test_split(X, y, random_state=42)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00002-29a302fa-c2aa-4d57-b09c-e571c3028851",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9eac2103",
    "execution_start": 1640269202352,
    "execution_millis": 16,
    "deepnote_cell_type": "code"
   },
   "source": "def train(features, labels, model, lossfunc, optimizer, num_epoch):\n\n    for epoch in range(num_epoch):\n        inputs = torch.from_numpy(features)\n        targets = torch.from_numpy(labels)\n\n        outputs = model(inputs.float())\n        loss = lossfunc(outputs, targets.float())\n        \n        optimizer.zero_grad()  \n        loss.backward()\n        optimizer.step()\n        \n        if epoch % 10 == 0:\n            print ('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epoch, loss.item()))\n        \n        \ndef visualize(x_train, y_train, model):\n\n    predicted = model(torch.from_numpy(x_train).float()).data.numpy()\n    predicted = (predicted >= 1/2).astype(int)\n    order = np.argsort(x_train, axis=0)\n    missed = np.absolute(y_te - output)\n    plt.plot(missed[missed > 0], 'ro', label='Misclassified data')\n    plt.legend()\n    plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## MLP",
   "metadata": {
    "cell_id": "00003-e705930e-52fa-499a-8bcb-1c1b65ab807a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-d12eaf1e-334f-436a-9dfe-2208e11f2929",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ed4dc71a",
    "execution_start": 1640269202374,
    "execution_millis": 47,
    "deepnote_cell_type": "code"
   },
   "source": "class MLP(nn.Module):\n    def __init__(self, hidden_size):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(133, hidden_size)\n        self.activation_fn1 = nn.Sigmoid() \n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.activation_fn2 = nn.Sigmoid() \n        self.fc3 = nn.Linear(hidden_size, 1)\n        self.activation_fn3 = nn.Sigmoid() \n    \n    def forward(self, x):\n        out1 = self.fc2(self.activation_fn1(self.fc1(x)))\n        out2 = self.fc3(self.activation_fn2(out1))\n        out3 = self.activation_fn3(out2)\n        t0 = torch.zeros(out3.shape)\n        t1 = torch.ones(out3.shape)\n        print(t0.shape)\n        return torch.where(out3 > 1/2, t1, t0)\n    \nhidden_size = 10\nlearning_rate = 1e-1\nlossfunc = nn.MSELoss()\n\nmodel = MLP(hidden_size=hidden_size)\n\noptimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00005-4bbc032c-a507-44c3-89fe-5974ab66d574",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4e41e5d8",
    "execution_start": 1640269202428,
    "execution_millis": 13,
    "deepnote_cell_type": "code"
   },
   "source": "def build_k_indices(y, k_fold):\n    \n    num_row = y.shape[0]\n    interval = int(num_row / k_fold)\n    np.random.seed()\n    indices = np.random.permutation(num_row)\n    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n    return np.array(k_indices)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-1d698c65-1b27-452f-874a-eff9c507c907",
    "deepnote_output_heights": [
     597.1875,
     597.1875,
     null,
     250,
     null,
     250,
     null,
     250
    ],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "260b2024",
    "execution_start": 1640269202447,
    "execution_millis": 767,
    "deepnote_cell_type": "code"
   },
   "source": "k_fold = 4\nlambdas = np.logspace(-4, 0, 30)\n\n# split data in k fold\nk_indices = build_k_indices(y_train, k_fold)\n\n# define lists to store the loss of training data and test data\nrmse_tr = []\nrmse_te = []\nfor k in range(k_fold):\n\n    te_indice = k_indices[k]\n    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n    tr_indice = tr_indice.reshape(-1)\n    y_te = y_train[te_indice]\n    y_tr = y_train[tr_indice]\n    x_te = x_train[te_indice]\n    x_tr = x_train[tr_indice]\n    \n    train(features=x_tr,\n        labels=y_tr,\n        model=model,\n        lossfunc=lossfunc,\n        optimizer=optimizer,\n        num_epoch=500)\n\n    output = model(torch.from_numpy(x_te).float()).data.numpy()\n    #output = (output >= 1/2).astype(int)\n    missed = np.absolute(y_te - output)\n    print(\"Accuracy:\", 100 * (len(y_te) - np.count_nonzero(missed)) / len(y_te) , \"%\")\n    visualize(x_te, y_te, model)\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "torch.Size([606, 1])\n",
     "output_type": "stream",
     "data": {}
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8c9d6cdfd2e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlossfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlossfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         num_epoch=500)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-bb80f35c7361>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(features, labels, model, lossfunc, optimizer, num_epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ],
     "data": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "00008-aa18d1e8-5936-4309-a567-62127f30cf74",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "00009-8e979dbd-23a8-49d0-b322-f39769ceae05",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d66177de-7ea1-46c8-aea6-26d701dd9bc9' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "d4a06343-16a0-4cd9-8fd2-2ee7aa0a2707",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 }
}